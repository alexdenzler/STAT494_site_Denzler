---
title: "Assignment #4"
output: 
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    code_download: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

```{r libraries}
#Regular expressions
library(tidyverse)        # contains stringr for regex
library(googlesheets4)    # for reading in data
gs4_deauth()              # to read in google sheet (or download)

#tmap
library(tmap)
library(pacman)
```


## Put it on GitHub

[Here](https://github.com/alexdenzler/STAT494_site_Denzler) is my GitHub link.

## Regular Expressions

```{r cache=TRUE}
bestsellers <- read_sheet("https://docs.google.com/spreadsheets/d/1n3xKHK4-t5S73LgxOJVJWT5fMYjLj7kqmYl1LHkpk80/edit?usp=sharing")
```

  1. Find books with multiple authors (HINT: Consider the possibility of an author having “and” in their name)
  
```{r}
bestsellers %>% 
  mutate(mult_auth = str_detect(author, "\\sand\\s")) %>% 
  filter(mult_auth == "TRUE")
```
  
  
  2. Detect if the author’s first name starts with a vowel
  
```{r}
bestsellers %>% 
  mutate(vowel = str_detect(author, "^[AEIOU]")) %>% 
  filter(vowel == "TRUE")
```
  
  3. Change all authors with the name Alexander to Alex
  
```{r}
bestsellers %>% 
  mutate(names = str_replace_all(author, "Alexander", "Alex"))
```
  
  4. Find books that are the second book in a series

```{r}
bestsellers %>%
  mutate(series = str_detect(bestsellers %>% pull(description), 
                             pattern = "second book")) %>%
  filter(series == TRUE)
```
  
  
  5. Find books that are the third or fourth one in a series
  
```{r}
bestsellers %>%
  mutate(series = str_detect(bestsellers %>% pull(description), 
                          pattern = "(third|fourth) book")) %>%
  filter(series == TRUE)
```
  
  6. Find books that are the 10th, 11th, …, or 19th book in a series
  
```{r}
bestsellers %>%
  mutate(series = str_detect(bestsellers %>% pull(description), 
                          pattern = "1[0-9]th book")) %>%
  filter(series == TRUE)
```
  
  
  7. Describe in your own words how you would go about writing a regular expression for password pattern matching (i.e. 8 character minimum, one capital letter minimum, one lowercase letter minimum, one digit minimum, one special character minimum)
  
  * Each regular expression would have to check to see if the range or requirements were included in the password. We would be able to use `str_detect()` and have each regular expression check for a capital letter, a lowercase letter, a digit, and a special character (whcih could be written like this $[a-z][A-Z]\d\W$). 
  
  
## `tmap` Exercises

* Read in the data

```{r}
data("World")
```

In addition to the World data, we will use data called **metro** that comes from the tmap package. It contains metropolitan area information from major cities across the globe. Some important variables include:

  * **Name**: City name
  * **pop2010** : population in 2010
  * **pop2020** : population in 2020
  
```{r}
data("metro")
```


  1. Make a world map using the base **World** dataset we used with the COVID example. Add information about  income groups (`income_grp`) to the plot, specify a color palette.

```{r}
names(World)
names(metro)
```

```{r}
tmap_mode('plot')

tm_shape(World) +
  tm_polygons("income_grp", 
              palette="-Blues", 
              contrast= .5, 
              id="name", 
              title="Income group")
```


  2. To the plot from (1), add the new dataset **metro** to the plot, and add information about cities’ populations in 2020

```{r}
tmap_mode('plot')


tm_shape(World) +
  tm_polygons("income_grp", 
              palette="-Blues", 
              contrast= .5, 
              id="name", 
              title="Income group") +
  tm_shape(metro) +
  tm_bubbles("pop2020",
             contrast=1, 
             title.size="Metro population", 
             id="name")
```


  3. Now, make a new plot with the World data that provides a look at country economic status and the inequality associated with each.
  
```{r}
tmap_mode('plot')

tm_shape(World) +
  tm_polygons("economy") +
  tm_bubbles("inequality", 
             border.alpha = .5) +
  tm_format("World_wide")
```
  
  
  4. Using a new data set, `NDL_muni` municipality data from the Netherlands, create a plot with two separate maps. One showing the percentage of men per municipality of the whole country, and one showing the same but faceted by **province**.
  
```{r}
tmap_mode("plot")

data(NLD_muni)

NLD_muni <- NLD_muni %>% 
  mutate(perc_men = pop_men / population * 100)

tm_1 <- tm_shape(NLD_muni) + 
  tm_polygons("perc_men", 
              palette = 'RdYlBu')

tm_2 <- tm_shape(NLD_muni) +
    tm_polygons("perc_men", 
                palette = "RdYlBu") +
    tm_facets(by = "province")

tmap_arrange(tm_1, tm_2)
```
  
  
  
## Data Ethics: Data visualization principles

I had heard of all of the principles from the article because I read *Calling Bullshit* for a class during Module 3 of this year. The graph that stood out as the worst to me was the one under the section "A changing denominator". There were many issues with this graph: first, there was too much information on the graph, which made it impossible to decipher. Second, there were a few different scales, which made it extremely difficult to focus on and interpret. Finally, each type of plot on the graph had tons of overlap, making it even more difficult to read. The graph that fooled me was the barplot that did not have zero included on its x-axis. This seems like a common mistake that even an experienced data scientist could make and gloss over. Data visualization is extremely important when discussing the topic of data ethics. Bad data visualizations can mislead the reader, and cause them to spread misinformation.





  
  


